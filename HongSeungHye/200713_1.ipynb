{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트 처리 관련. 3년째 맡고 있음. 정보창의연구소 연구교수. 석사 때는 정보검색 시스템, 특히 자동질의시스템, Q&A 시스템. 딥러닝 이전. 당시 하던 게 시멘틱. 사람을 잘 매칭시켜주는 법. 나쁘게 얘기하면 매칭 사이트. 당시 여러 가지 썼음. 추천 등 사용. 박사 때는 감성분석. 딥러닝 안 좋아함. 특히 텍스트 처리 시 어휘 위주로 해서. 그 쪽... 연결주의자. 머신러닝, 특히 NN 좋아하는 사람들, 어느 순간 대세가 되어서 씁쓸함. 주로 어휘 관련 가르쳐드릴 거고, 딥러닝은 교수님 및 선동언 선생님이 진행.\n",
    "- 오늘은 12시까지만 하고, 오후에는 특강. 1시부터. git. git 중요. 오픈소스이기도 하고, 개발하려면 중요하고. 플젝도 올려야 하고. 머신러닝 등이 올라가 있으니 직접 만들지 않아도 되고. 다만 본 수업에서는 직접 만들 것. 코드 많이 칠 것. -> 다음주 온라인 수업 우려. 화면 보는 것 힘들 것. 염두에 두기.\n",
    "- 50분 하면 무조건 10분 쉴 것.\n",
    "- 조교: 과정 선배. 작년 수강생. 동일 과정 겪었으니 조교 도움이 더 현실적으로 도움 될 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 주 자료 올려둠.\n",
    "- 기계학습, 인공지능 과정 아님. 빅데이터를 잘 활용할 수 있게 교육하는 사업. 다만 그 중 빅데이터 어떻게 활용? -> 인공지능 이용 활용, 교육과정 구성, 선발.\n",
    "- 빅데이터에 대한 간략한 개요.\n",
    "- 빅데이터: 정형, 반정형, 비정형 데이터. 이번 주 모두 진행. 데이터의 종류 알고, 컨트롤 방법, 가져올 방법 등 알아야.\n",
    "- 다음 주: 크롤링. 크롤링: 자연어 처리 기술의 하위에 속함. 정보검색의 중요 분야. 이를 바탕으로 구글이 만들어짐. 어설픈 지식이 아닌 목적, 중점, 사이트 간의 관계를 통핸 정보의 가치 탐색 등을 보고, 스크래핑을 진행 예정. 크롤링 =/= 스크래핑. 페이지를 어떻게 뜯어오고, 비정형 데이터를 어떻게 가져와 처리할 지. 여러 기법 있고 특히 빅데이터에서 비정형 데이터는 DB에 들어가지 않는, 일정한 형태가 없는 소리, 텍스트... 자연어를 어떻게 처리할 것인가 -> 자연어, 어휘 위주 처리 예정. 계획중인 건 딥러닝, 타 머신러닝 알고리즘까지 갈 수 있게 설계를 하는 법. 텍스트를 기계에 넣어 학습시키기 위해 텍스트를 임베딩 시키는 수 십 가지 방법 중 대표적인 것. 그리고 시간 남으면 정보검색 방법. 특히 정보검색에서 중요한 것: 비정형 데이터에 가중치 매겨서 중요한 것 판별하고 사용자 니즈 찾기. -> 검색, 추천, 분류에 사용 가능.\n",
    "- 인버티드 인덱스, 역색인 -> 맵리듀스 -> GFS -> HDFS -> 빅데이터 시대 도래."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 오늘: 빅데이터에 대해 간단히 이해."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 빅데이터: 맥킨지 정의를 따름. 아주 큰 규모의 데이터. but 만나볼 수 없음. 빅데이터는 금융권 취직하면 볼 수 있음.\n",
    "- 빅데이터를 어떻게 생각해야 할 지, 가치를 어디에 둬야 할 지, 어떻게 다뤄야 할 지(핸들링) 위주.\n",
    "- 실제 데이터 규모에만 초점 맞추는 것 아니고, 데이터 수집, 저장, 관리, 분석 전 과정 = 빅데이터. 여기서도 수집, 저장 관련을 데이터 엔지니어링이라고 함. 분산 처리, 저장 후 가져오는 법. 분석에서는 전통적으로 통계적 기법 다수 사용. SPSS, R 등으로 분석. 저 데이터를 컴퓨터에서 사용할 수 있게 됨, 특히 비정형 데이터 -> 머신러닝 쪽으로 접근. 데이터 어낼리스트, 데이터 엔지니어, 인공지능... 이런 식으로 길이 나뉨. 우리는 그 중 인공지능 활용 및 산출물까지.\n",
    "- 빅데이터: 3V. 데이터의 양, 얼마나 다양한 종류인지(과거에는 숫자. DB에 투입 가능. 나이, 한 주 커피... 전부 숫자. 현재는 달라짐. 카페에서 자연어 입력 -> 누군가 분석. 대부분 알바가 스프레드 시트에 정리. 그런 걸 어떻게 텍스트 처리할 지.) 생성 주기 매우 빨라짐. SNS의 등장으로 인스턴스 메세지, 트위터 등의 단문이 버려지면서 누군가 관리하고, 그 안에서 유용한 가치를 찾고자 함. ex) Facebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 규모에 따라 분류: 스몰, 미디움, 빅데이터.\n",
    "- 스몰 데이터 - 엑셀. 통계분석으로 커버 가능.\n",
    "- 일정 규모 이상: DB 필요. 데이터 엑세스, 처리 시간 오래 소요. 우리가 볼 것이 Large Data. DB 사용. 정형 데이터 -> SQL로 쿼리 날려서 데이터 가져옴. 데이터 간의 관계. 전통적 RDBMS 등 데이터 관점에서 구조화, 관계 분석 위주로 진행.\n",
    "- 문제: 앞으로 해야 할 것. 빅데이터는 데이터의 형태가 구조화되어 있지 X. 이미지, 소리... -> Feature를 찾음. 필터를 입혀서 벡터를 찾음. 자연어도 마찬가지. 텍스트를 어떻게 특징으로 만들 수 있는지. 벡터를 이용, 네트워크 학습. = 딥러닝.\n",
    "- 저장, 관리 관련: 하둡, 스파크 이야기 나온 조 있음. 재작년에는 여기서 스파크 다뤘는데, 관심 없고 어려워함. -> 뺌. 뭔지만 다룰 것. 스파크, 하둡이 뭔지.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 중요해졌고, 많이 생기고 있고, 인스턴스 메세지, 클라우드 플랫폼으로 데이터 사용, ... 가치를 찾고 인사이트를 찾는 데 주목. 데이터의 관계보다 한 단계 위, 데이터에 숨겨진 것. 아마존, 넷플릭스 등 예제.\n",
    "- 리캡챠: 회원가입, 예매 등 할 때 자동차 고르거나 숫자 입력. bot과 사람 거르는 목적. 해당 정보를 빅데이터와 연결, 머신러닝 발전에도 연결. 쓰고 버리는 데이터 -> 집단지성을 이용, 유용하게 사용하고자. 초기에는 텍스트로 고서 번역. 정상적 답변 -> 나머지도 정상 답변이라는 통계 기반, 정상 데이터와 기계가 못 읽은 이미지. 나머지도 아마 정답일 거라고. 고서 디지털화와 글자 태깅에 사용. 데이터 인풋과 아웃풋 완성 -> 기계학습 모델 구성 가능. 이게 발전해서 자동차를 고르시오. 그 중 일부는 자동차. 태깅됨. 이 작업 필요한 이유: 데이터로 모델 만들려면 무슨 데이터인지 기계에게 알려줘야. 데이터는 아주 많은데, 해당 데이터 이용 위한 환경이 갖춰지지 않으니 알바 사용 -> 인력과 자본 소모 -> 리캡챠 생성.\n",
    "- 자율주행차, 기계의 사물인식, 가치 창출에 이용해야 함. 더 많은 노력, 관심.\n",
    "- 빅데이터의 효율적 관리 관심 -> 하둡 등. 데이터 관리, DB 관련 자격증 따면 됨. 고연봉. but 정형 데이터 비율이 전체 데이터 중 매우 적음. 차라리 분석이 나을 수 있음.\n",
    "- 대학원은 잘 생각해보고 가기. 노예가 몇 명 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아마존, 넷플릭스 등 사례는 각자 보기. 중요하지 않고, 어려운 내용 아님."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전체적인 빅데이터 분석 flow.\n",
    "- 데이터 수집, 관리, 분석, 시각화까지. visualization, 찾은 인사이트 활용. 전체 과정을 봤을 때 파이썬을 중심으로 진행. 무슨 데이터가 있는지 수집 저장, 그 다음 주 분석, 프로젝트로 결과물 산출. 이 flow대로 진행.\n",
    "- 진행 과정에서 알아야 할 것: 수집 관련 기술 - 크롤링, 스크랩핑 등. 크롤러 만들 것 -> 많은 데이터 생성. 문서, XML, JSON, HTML, 사진, JPEG, GIF, PNG, BMP, 소리, 전통적 DB의 정형 데이터...\n",
    "- 그런 데이터를 수집, 어딘가에 보관해야. 파일로 가지고 있을 수는 없음. 몇십만 단위 넘어가는 데이터. 데이터 사이즈만 n GB이기도. -> 관리 필요. 본래 빅데이터는 분산 환경(클라우드 등)에서 효율적으로 관리하는 방법, 전통적 DB로는 안 되는 이유 등 다룰 것. 대부분 빅데이터 = 하둡 의미. 아파치 최상위 플젝 중 하나, 무료. 수정 가능. 비용도 저렴. 분산. IDC에서 컴퓨터 임대 불필요. 하둡의 가장 큰 강점 중 하나. 분석 아닌 저장 용도. 하둡, GFS의 핵심 알고리즘: 맵리듀스.\n",
    "- 우리는 안 할 것. 소개만 하고 넘어가고 전통적 DB를 배울 것. 그리고 역색인inverted index 구조를 정보 검색에서 배울 것. 그리고 거기서 맵리듀스가 나옴.\n",
    "- 분석: 기계학습, 통계적 방법, 경험적, 실험을 통해... 여러 방법 존재. 주로 통계 이용. 복잡한 통계식, t검정, 카이스퀘어 등 안 세고, 통계 기반의 확률. 굳이 어렵다면 조건부 확률, 조건부 독립 정도.\n",
    "- 데이터 수집, 관리, 가공해서 모델 생성해서 학습 또는 분석 처리 완료 -> 처리된 결과로 플젝의 주어진 문제 해결을 위해 시각화 또는 서비스 방안을 웹 형태로 완성. HTML, CSS, JS... 등 필요 가능하나 빅데이터의 데이터 다양성 측면에서 문서 타입 중 하나. -> 어떤 데이터 수집하고, 어떻게 데이터 생겼는지 공부하는 과정 = 지능정보 시스템 만드는 데 일조할 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 수집하려면 수집, 저장, 관리할 데이터 종류 알아야. 정형, 반정형, 비정형으로 분류 가능.\n",
    "- 정형: 전통적 DB에 들어가는 것. 엑셀에 들어가는 애들. 날짜, 통화, 숫자, 부동소수점, 주민등록번호... 표기 형식만 다를 뿐 숫자. 주로 숫자를 다룸 -> 통계적 접근 방법 유용하게 사용.\n",
    "- 반정형: aka document type. HTML, XML, JSON 등. 주로 메타데이터라고 부름: 실제 데이터보다 문서를 표현하기 위한 도구로서의 언어로 기술됨. markup 언어. 태그라는 형태로 구성. 데이터를 많이 잡아먹음. -> JS에서 쓰는 object 표현 방식에서 나온 json 다수 사용. json도 다룰 것. -> 오픈 API, 공공 데이터 포털, 정부 제공 API 활용법 등 다룰 것.\n",
    "- 비정형: 영상, 스틸컷, 소리, 텍스트... 우리가 다룰 빅데이터. 그 중 텍스트 위주로 이번 주 과정 진행. NLP 관심 있다면 생각보다 손이 많이 가고 데이터 정제가 힘듬.\n",
    "- 할 일\n",
    "    - 정형 데이터 -> DB\n",
    "    - 반정형 -> HTML XML JSON CSS(시스템 구축 위해)\n",
    "    - 비정형 데이터: 분석 관련. 전처리 필요. 전처리 중요. 모델 구성 안 하고 케라스 사용만 할 수도 있음. 데이터 인풋단 설계, 텍스트, 이미지, 소리 데이터 처리 방법 비슷하지만 다름. 특히 텍스트 데이터 처리 시 언어적 지식 필요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 비정형 데이터 비중 엄청 오르고 있음 -> job을 찾거나 기술, 능력 개발 시 DB 취업보다는 비정형 데이터 다루는 것이 경쟁력 있을 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 크롤링, API 사용 할 것. 회사에서는 로그 수집기가 중심; 회사에 DB 존재. 해당 데이터 사용법 중요. DB 관리자들이 엑셀, csv 형태로 제공. 다루긴 할 것. 파일 형태의 DB, SQL lite로 해볼 것. 주로 할 것: 비정형 데이터를 합법, 불법적 수단으로 가져오는 법.\n",
    "    - 크롤링 시 조심할 점 존재. 다룰 것.\n",
    "    - 데이터 3법 언급 예정.\n",
    "- API: 공공 데이터 포털에서. 데이터 진흥원 데이터도 가져다 써도 됨. 쓰는 방법은 비슷할 것.\n",
    "- RSS 리더: 과거 기술."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 저장, 관리 법. 대부분 분산 파일 시스템, 하둡 또는 NoSQL. key value 쌍으로 이루어짐. 특정 구조 없음. 데이터를 저장, 가공해서 사용. 모든 데이터 저장 가능. 분석가가 골치.\n",
    "- NoSQL 제외 3개는 전부 통합되어 같은 이야기.\n",
    "- 우리는 이 중 제일 쉬운, DBMS 중 RDBMS.\n",
    "- 파이썬, 객체지향, DB 연결법 알아야 -> ORM, object relational mapping, DB의 객체화 기술 존재.\n",
    "- 하둡, 스파크: 오늘 잠깐 소개."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실시간 처리 기술에 스파크 포함. 하둡 =/= 스파크. 모두 아파치 프로젝트. 스파크는 금융권 희망 시 알고 있으면 도움 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 마이닝. 데이터에서 유용한 정보 산출하기 위한 데이터 마이닝 기법으로 기계학습, 통계적 기법 등. 기계학습에서는 특히 NN base 딥러닝과 전통적 방법 등 사용. 다 알아야.\n",
    "- 군집화, 분류..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하둡과 스파크\n",
    "- 일반적 빅데이터 엔지니어링 과정에서 배움.\n",
    "- 스파크: 원래 하둡 서브 프로젝트. 하둡 위에 올라가는 작은 요소. 통신 기술 발전 -> 실시간 분석 needs 증대 -> spark 가치 증대 -> 독립 프로젝트로 분리, 이제 별개. 하둡 없이도 스파크 구동 가능.\n",
    "- 하둡: 맵 리듀스 알아야. 함수형 기법에서 이미 봤을 것. 조금 다를 수는 있음.\n",
    "    - 필요한 이유: 분산환경. 각 컴퓨터에게 일 시켜야 -> 일 시키는 방식. 과거: master, 전체 서버에서 데이터 취합하거나 일을 분산. 이제는 해당 기능이 약해짐. 각 컴퓨터에서 일하고, 그 대표적인 게 맵, 리듀스.\n",
    "    - 구글은 검색 회사; 텍스트 처리 -> 맵리듀스로 흘러감."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하둡 에코 시스템. 진한 색이 하둡. 하둡 = DB. 특히 분산된 형태로 저장; 하둡 File System. 논문 기반으로 만들어진 시스템 중 관리는 하둡, 딥러닝에서는 텍스트 처리 시 Word2Vec, 크롤링, ...\n",
    "- 나머지: 저장소 옆에 붙는 시스템. 기계학습, 분산환경에 쿼리 날려 작업... 서브 프로젝트가 붙음. 그 중 하나가 스파크였음.\n",
    "- 안정적 데이터 관리. 장점: 분산 환경에서 네트워크만 깔려 있으면 추가 가능. 리눅스에서 돌아가고, 자바로 되어있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 스파크. HDFS 아니어도 되고 일반 데이터, DBMS도 가능.\n",
    "- 주 역할: 머신러닝. 특히 realtime으로. 금융권에서 사용 이유: 거래 이력 매우 많음. 그 중 위험 감지, 부정사용 탐지 필요 -> realtime 추적 필요. 또는 주가 등락 분석. 금융권 희망 시 스파크 알아야. 알고 있으면 경쟁력 상승."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하둡과 스파크는 다르다. 하둡은 저장하는 DB, 스파크는 분석용.\n",
    "- 맵리듀스 쓰는데 둘의 차이: 하둡은 디스크 사용. distributed storage; storage = 물리적. spark는 메모리 사용. 속도가 빠름 -> 실시간 처리 가능. 하둡 실시간 처리를 위해 만들었다가 따로 떨어져 나간 관계.\n",
    "- RDD는 관심 있으면 보기. 작업하는 단위."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 맵 리듀스 설명 대표적 예제. 이 쪽에 있는 걸 빅데이터라고 생각하자. 매일 SNS 글 올라옴 -> 글을 분석. 블락이나 검색 서비스 제공 등. 3군데 컴퓨터에 분산.\n",
    "- map -> key value 쌍으로 만들어줌. reduce -> 합침. -> 전체 데이터 한 번에 처리한 것과 같아짐. 각 서버에서 검색하기 위한 것 만들고, 거기서 해결 가능한 건 캐시에 올려 해결하고, 그 외는 시간을 더 들여서 fianl result 쪽을 검색. 이를 위해 구글이 만듬."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터, 특히 여기서 얘기한 것: 정형 데이터, 반정형 데이터, 비정형 데이터가 중요. 처리 위한 일반적 빅데이터 도구는 하둡, 스파크 등."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이게 왜 필요할까? 왜 에 대한 질문이 해결되어야. 필요성.\n",
    "- 필요성만 있다면 어려운 알고리즘 아니고 단순 통계 기법이어도 괜찮음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 비정형 데이터가 중요. DB는 알고 있고, 활용할 줄만 알면 됨. 이 쪽으로 진로 나갈 게 아니라면.\n",
    "- 참고로 DB 쪽은 그럼에도 불구하고 돈 되는 분야여서 SQL 쿼리만 최적화해주는 사람들 있음. 극소수."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DB. 이제 정형 데이터보다 비정형 데이터가 전체 데이터 비중 더 많이 차지. 약 80%. 3년 전 기준. 지금은 비율 올랐을 것. 비정형의 대표적 예시: 영상, 이미지, 소리, 텍스트. '혁신'이라고 성과 이룬 걸 보면 이미지 인식, 자동 스크립트, 음성인식, 이미지 오브젝트 detecting 등 비정형 쪽. 빅데이터 시대에 비정형 데이터 중요. 비정형 데이터 어떻게 다룰 지 잘 알아야.\n",
    "- 그럼에도 불구하고 정형 데이터 중요: 지금까지 해온 것. DB는 없을 수 없음.\n",
    "- 관심이 비정형으로 넘어감 -> 각 기업에서 빅데이터 활용 가능자, 분석 가능자, 기계학습, 딥러닝으로 플젝을 완성해본 사람을 우대. 리쿠르팅 우대 사항. 직무 무관 코딩 시험 등.\n",
    "- 다른 시각의 아이디어가 구현될 때 좋은 아이템이 나옴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우리가 할 것: 데이터를 얼마나 잘 관리하느냐, 만드느냐보다 숨은 가치 찾기. 데이터와 정보를 구별, 정보를 잘 만들어내는 것에 관심 많음. 이유: 그런 정보를 가지고 의사결정에 이용. 감성분석 등에서 주로 하는 게 SNS 수집, 분석. 갤럭시 신제품 출시 시 시장 반응, 사람들의 선호, 긍정적 평가 등을 기업이 궁금해함. -> 그에 맞춰 홍보 및 차기작에 반영. 기업, 회사의 의사결정에 매우 중요한 가치 제공. 또는 주식 매매를 결정. 타이밍 예측, 결정 요인이 될 수도. so 단순 데이터보다는 정보에 관심 가져야. but 그런 정보 만들기 위해 데이터 필요. 데이터 효율적 관리, 저장되어 있어야. so DB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DB의 제일 중요한 점: integrated. 파편화된 데이터의 조직화, 관리, 필요할 떄 사용 가능하도록.\n",
    "    - 잘 조직, 관리: 중복되지 않게끔. missing data나 오표기, 중복 등 걸러내야 함. 데이터 중복 = 불필요한 컬럼 존재 -> 분석 입장에서 연관이 생기거나 불필요한 것으로 인해 모델에 약영향 가능. DB 관리 측면에서는 조직 안 되는 경우 발생 가능.\n",
    "- 관계형 DB: 데이터들 파편화; 서로 관계 있게 관리. 관계를 이용, 정보를 찾는 형태. 객체로 관리하기도. 여러 가지 있음. 그 중 제일 대표적, 많이 쓰고. ERD를 보면 시스템 전반 파악 가능.\n",
    "- 저장, 공유, 운영 중 공유: 파일 DB 쓰지 않는 이유 중 하나 = 동시에 여러 명 작업 불가. 특히 DB는 실시간, batch 단위 데이터 관리하다 요청하면 리턴해줘야 하는데 그게 불가능. DB가 가져야 하는 덕목으로 이런 게 있음.\n",
    "- DB 관리 시 해당 덕목 몰라도 됨. 우리가 공부할 건 DB가 아닌 DBMS. 데이터베이스 매니지먼트 시스템. 이런 필수 요소, 조건을 갖고 있어, 얘들을 관리해줄 것. DB =/= DBMS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 저장 관련 조금 더. 실시간으로, 내가 원할 때 사용 가능해야. 실시간 접근 가능해야.\n",
    "- 계좌이체 등 하는데 DB에 기록이 안 된다면 돈을 꺼낼 수 없음. DB의 데이터가 지속적으로 현재 상태로 관리되어야. 이런 게 가능해야; DBMS가 해줌.\n",
    "- 내용에 의한 참조 중요. key값이 아닌 value로 찾아야. 대표적 예: 게시판 검색. 내용에 의한 참조 -> 해당 값 있으면 불러옴. so 정보 검색 별도 필요. 키워드 잘못 넣으면 없다고 나옴. 가격 비교시 키워드 안 맞으면 안 나오는 서비스 존재. 내용으로 데이터 찾아줌. 우리는 DB를 가지고 특정 조건, 시기, 형태의 데이터를 불러다가 쳐낼 거 쳐내고 전처리해서 모델에 집어넣을 것. then 다른 종류의 정보, decision making에 필요한 정보 나올 것. DB는 단순히 데이터를 잘 저장하고 불러오기 위한 목적. DB는 그런 역할을 잘 수행할 수 있게 필수 요건을 갖고 있는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DBMS: 프로그램의 집합. DB를 자동으로 관리 가능하게 프로그램화 되어있음. 하나의 시스템; 독립적인 unix, linux의 데몬(혼자 돌아가는 프로세스의 총칭), 웹서버가 필요하듯 논리적 서버가 필요.\n",
    "- 우리는 설치 X 위해 파일 DB 쓸 것. 대표적으로 SQLite. 앱에서도 많이 사용. 안드로이드에는 패키징되어 있음.\n",
    "- DBMS의 역할: DB 관리 -> 여러 개의 DB 관리. DB 생성, 쿼리, 삭제, 동시 접속 위한 사용자 권한, 사용자 생성, DB 연결, 데이터의 구조, 관계형 DB라면 테이블 간의 관계(카디널리티) 등 담당.\n",
    "- 실제 DB는 밑단에 있고, 전체를 관리하는 DBMS가 돌면서 요청한 것을 전달. 즉각 응답 위해 독립된 형태의 프로세스로 존재: '서버를 띄운다'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DBMS가 해주는 것. DB의 요건을 충실히 수행해야.\n",
    "    - 공유: 여러 사람이 와서 access 가능.\n",
    "    - 필요할 떄 데이터 접근\n",
    "    - 데이터 안정적 보관 가능하도록 관리\n",
    "    - 데이터 입출력에 있어 유지 가능하도록 보존\n",
    "    - DB 뚫리지 않게 보안\n",
    "        - 과거 SQL injection이라는 해킹 기법 흔했음. ID PW에 쿼리 써넣으면 뚫렸음. 그런 걸 방어하기 위한 security 관련 모듈, 함수, 프로시저 등 존재.\n",
    "    - DB 관리 시 missing data, 부정확 데이터, 자료형 불일치 등 발생하면 안 됨. 이를 적절히 오류 처리. syntax error처럼.\n",
    "- 문제: DBMS는 독립 프로그램 집합; 제조사마다 다름. 오라클 쿼리가 MS SQL에서 작동 않을 수 있음.\n",
    "    - 친숙한 DB에 맞춰서.\n",
    "    - MySQL, MariaDB, SQLite... 각각 지원 SQL문 다르고 기능 다름. 가장 가벼운 형태가 SQLite니까 SQLite만 사용 예정.\n",
    "- 아래 그림: 전처리 관련. 중복 피하고, missing 없애고, label 잘못된 거 없애고, outlier 제거 등."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DBMS. 선택지 많으나 RDB, 관계형을 다룰 건데, 그 외에도 다수 존재. 특히 object로 하는 경우도 있고, 계층형, 네트워크형... 각 장단점 있음.\n",
    "- 관계형 DB: 저장되어 있는 데이터들을 어떻게 관리할 지. 관계형으로 관리 예정. 여기서는 개체, entity와 entity 사이의 relationship을 가지고 테이블 구조 설계. 스키마. -> 데이터 중복 피하고, 데이터 파편 잘 관리할 수 있게 하는 것 = RDB.\n",
    "- 비슷한 것: 객체. 객체를 그대로 DB로 넣을 수 있음. then 객체지향 시 편함. RDB를 가장 많이 쓰지만 수요 있음 -> ORM까지 할 것.\n",
    "- 실제로는 RDB로 들어가 있지만 꺼낼 떄는 객체로 가져올 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDBMS: DB가 갖춰야 하는 조건, 기능과 겹치니 스킵.\n",
    "- 모든 DBMS가 마찬가지지만 fault tolerance해야 함. 오류 발생 시에도 DB 멈추면 안 됨. 다행히 우리는 롤백 기능만 쓸 줄 알면 됨.\n",
    "- SQLite: FileDB -> 혼자 씀 -> Fault Tolerance 무관. 멈추면 재가동, 오류는 내가 수정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDB에서 흔히 테이블이라고 부르는 애를, 다른 형태로 부름. 'Relation'. 관계를 가질 것.\n",
    "- 각 테이블에는 데이터 들어있음. 모델의 input으로 쓰이는 dataset이라면 feature, instance 있음. 다르게 부름. row 대신 tuple. column = attribute. feature의 수 = degree. PK, FK 등 쓸 것.\n",
    "- Primary key: 테이블과 테이블의 관계를 가질 때 누가 누구를 찾아가야 할 지 모름. 그 떄 관계 역할을 해주는 게 PK. 유일해야 하고, 절대 겹쳐서는 안 되는 키. 내부적으로 해시 테이블로 관리 -> access 빠름. hash화해서 저장하는 데이터 배웠을 것.\n",
    "- 컬럼 5개 -> degree 5개. 첫 attribute = studentid = PK = 고유함. PK 설정 시 숫자로만 이루어져 있는 게 좋음. 문자보다 숫자가 나음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 릴레이션 이름 = 테이블 이름 = student\n",
    "- degree = 4\n",
    "- stu_no가 PK일 것. 겹치지 않으니. 숫자로 구성.\n",
    "- stu_name: 2번쨰 어트리뷰트. 문자로 구성. 데이터 타입이 stu_no와 다름.\n",
    "- 영어이름, 전화번호... 옛날식.\n",
    "- 첫 튜플 PK값: 20001001.\n",
    "- 이름이 중요한 게 아니라, 이렇게 생긴 테이블이 모인 게 DB. DB를 관리하는 게 DBMS. 특히 이런 식으로 테이블 간 관계를 맺어 관리하는 걸 관계형 DBMS, RDBMS라고 부름."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 보통 DB 설계. 혼자 코드 짤 수 없음. 협업 필요. 통용하기 위해 이해 가능한 형태의 언어 필요. 이를 문서라고 함.\n",
    "- 문서 중 특히 RDB에서 문서 주고받을 떄 쓰는 게 ERD가 대표적. 데이터의 구성을 나타내주는 다이어그램. 이 떄 필수 요소: 개체, 관계.\n",
    "- attribute, PK 설정 등 필요.\n",
    "- 엔티티는 네모, 관계는 <>. attribute는 동그라미. 관계, 엔티티 모두 어트리뷰트 붙을 수 있음. 특히 어트리뷰트 중 밑줄 있는 것 = PK.\n",
    "- 데이터 관계형 해석 시 필요한 것들."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학교 DB 설계 예제. 학교 구성원 존재. 구성원 간 관계. 이 학교에서 어떻게 엔티티 간 관계... 해석 가능.\n",
    "- 컴퓨터 관점에서 데이터 바라볼 것.\n",
    "- 엔티티는 주로 명사로 끝남. 데이터의 주체가 되는 애. 학교 -> 교수, 학생, 과목, 학과...[] = 엔티티. 데이터 구성에 있어 가장 중심이 되는 것.\n",
    "- 각 엔티티 간 관계 존재. 교수는 학생을 지도. 교수는 강의를 하고, 학과는 여러 관계를 맺고 있음. 소속.\n",
    "- 데이터와 데이터의 구성을 알 수 있음.\n",
    "- 엔티티가 테이블이 될 수도, 관계가 테이블이 될 수도 있음. 각 테이블, 릴레이션에 몇 개의 degree가 필요한 지 뽑아야. then 교수가 갖고 있어야 할 어트리뷰트는? 사번, 이름, 연구실 번호, 전화번호... 학생도 비슷할 것. 절대 데이터가 겹치지 않게 하는 게 좋음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 관계를 할 때 카디널리티: 관계 1:1도 가능하지만 1:n도 가능. PK, FK가 여기서 결정. 이를 나중에 오브젝트 형태로 뽑음; 누군가는 parent, 누군가는 child, 또는 서로 객체 부를 떄 단일 객체가 될 수도, list에 넣어놓을 수도 있음.\n",
    "- 카디널리티: min값 max값. 정규식 할 떄도 쓸 것. 거의 다 비슷하긴 한데.\n",
    "- 교수는 한 명인데 지도교수 하나에 속한 학생은 여럿 가능; 카디널리티는 1:n의 관계. 보통의 경우 한 학생은 한 명의 지도교수를 가지니 학생 입장에서는 1:1의 관계. then 교수 테이블, 학생 테이블, 지도 테이블이 있을 거고, 지도에는 FK만 가득할 것. PK는 없을 거고. 데이터 중복을 피하기 위해. 또는 view table로 만들 수도."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 조금 더 확장. 어떤 어트리뷰트가 필요한 지."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ERD. 안 그리는 사람도 있음: 그 대가는 시행착오 거치면서 코드 짜면 됨. 테이블이 100개 넘어가면 ERD 안 그린 것 후회하게 될 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 연습문제. 설명되어 있음. 관계형 DB를 통해 공급점, 각 부품의 재고와 판매 상황을 의뢰받아 설계한다고 가정. 원래 엔티티부터 찾아야 하지만 4개의 테이블 명시. 관계만 걱정하면 됨.\n",
    "- 부품점, 도시, 갖고 있는 부품, 얼마에 팔고 있는지... 지점마다 가격 다를 수 있음. 행사 등. 이를 고민 않으면 전국 동일 가격. sells라는 테이블까지. 4개의 테이블.\n",
    "- 어트리뷰트 총 3개. 컬럼에 필요한 이름도 있음. 4개의 테이블, 각 필요한 컬럼 -> 저대로 설계. 나중에 DB에 가서 테이블 이름 정하고, 컬럼 수, 데이터 타입 등 정해서 생성만 해 주면 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ERD 그려봄. 4개의 테이블. 명사형으로 끝나는 애들. 각 테이블에는 어트리뷰트가 서로 다르게 설정. 선의 방향 보기. 둘 사이의 관계.\n",
    "- city, supplier는 각각 부품을 판매ㅏ는 판매점인데, 어느 도시에 있는지. 관계를 맺고 있다는 뜻. 그걸 도시 입장에서 보면 한 도시에 여러 부품 공급점 존재 가능 -> 1:n의 관계.\n",
    "- city table: 도시의 이름. RDBMS의 요소에 PK 중요하다고 했음. 도시명은 PK가 될 수 없음. 월곡동이 전국에 3군데. 이게 PK가 된다면 중복. 제약조건 중 PK가 가져야 하는 덕목, unique함 상실 so PK 안 됨. PK = CNO. CNO에 의미는 없음. CNAME을 못 쓰니까 다른 걸 쓰는 것. 아마 연번일 것. PK 설정 후 auto increment하면 데이터 입력 시 자동으로 이전번호+1로 순서 만들어짐. 정수형 PK일 것. 엑셀로 치면 subsheet 하나가 끝남. 도시만 관리.\n",
    "    - 여기 있는 도시명 데이터는 다른 데서 안 쓰임: 중복 피함.\n",
    "    - 파일 헤더에 확장자, 인코딩, 전체 길이 등 들어있음. C로는 구조체 읽은 후 바이트로 해석. 파이썬은...\n",
    "    - 1:n의 관계 포함. * 은 0개 이상. 도시는 있으나 해당 도시에 지점 없을 수도.\n",
    "- supplier: 동일. SNO가 PK일 것. 실제 데이터는 SNAME일 것. then 몇 호점인지 이름 있을 거고, CNO가 FK. 참조키. 부품점 요소: 지역 정보 필요. 무슨 부품 팔고 있는지 필요. 정보 참조 위한 것.\n",
    "- sells: 실제 판매 이력. 또는 얼마에 팔 건지 계획해둔 거래 장부. 똑같이 PK 있을 거고, 어떤 부품인지... 카페라면 메뉴가 뭔지. 해당 메뉴가 얼마에 팔리고 있는지.\n",
    "- part: 진짜 메뉴에 대한 것. sells의 PNO도 FK일 것.\n",
    "- 어떤 데이터 join하냐에 따라 정보 달라짐. city와 supplier 참조 by CNO -> 특정 지역 가게 확인 가능. 해당 지역 상품 판매량: 3개 join. PK, FK로 테이블 묶기. 묶으면 1:n의 관계 -> 묶으면 1번 컬럼에 서울, 2번에 안암 1호점. 다음 row에 서울, 안암 2호점. 그게 이 두 테이블로 가능. 안암점 메뉴 리스트는 supplier, part 묶어야 하는데 둘을 직접 못 묶으니 sells까지 묶어야. 부품에 대한 직접적 PK값이 없으니. sells 테이블에 어떤 가게에 어떤 메뉴라는 정보가 있으니.\n",
    "- 데이터 양 줄이고 중복 피해서 보고 싶은 데이터만 보고 필요 없는 정보 지우게 컨트롤 가능. 관계형 DBMS는 이런 식으로 설계, 데이터가 어떻게 돌아가는 지 볼 수 있음. 그 중에서도 관계까지 설정해주면 더 좋고."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실제 데이터를 입력했을 떄. city 테이블. PK 지우면 도시만 있음. 같은 속성을 가지는 데이터만 넣어둔 것.\n",
    "- 어트리뷰트, 여기서 어트리뷰트는? 2개 있음. degree = 2. CNO와 CNAME. CNO 중간에 숫자 오고 있는데 문자 오면 안 됨. 어트리뷰트는 같은 데이터 타입. array와 비슷. 2개의 array를 딕셔너리에 key:value 쌍의 형태로 묶어놨다고 생각하면 됨. 쉽게 얘기하면.\n",
    "- 잘 봐둬야 하는 이유: 뒤에 데이터 타입 나옴. 데이터 타입 제대로 못 맞추면 벌어지는 일? 요즘 게임은 64비트인데 과거 게임은 버그 많았음. 돈을 1원을 떨궜다가 다시 주우면 2의 보수를 쓴다면, 0에서 -1이 되는 짓을 반복적으로 하다 보면 overflow 발생하며 max 찍기도. 고전 게임에서 hack이 가능한 이유: C에서 포인터 넘겨받아 접근 못 할 공간에 접근. 메모리 핵, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- supplier table. CNO 1 = 런던 -> SMITH 가게는 런던에 있는 것.\n",
    "- 중요한 건 어떤 파트를 얼마에 팔고 있는지 나오는 sells. 데이터 이렇게 생김. so PK, FK를 이용, ERD를 이용해서 해석 필요. 부품명, 가게명, 가격. 궁금한 것: 예를 들어 아메리카노 가격 전국 조사 -> PNO = 1을 뒤지면 10원, 11원... 지점이 다 쌈. 1번, 3번 매장이 쌈. 뒤져봐야 함. 아니면 한글로 다 써두면 DB 쓸 이유 없고, csv 저장이 나음. 데이터 분해 X so 원하는 형태로 불러올 수 없음.\n",
    "- 테이블이 어떻게 생겼는지, 컬럼이 어떻게 구성되어 있는데, PK와 FK의 관계, 테이블 설계 운영 시 데이터의 구성.\n",
    "- 이미지 수집 시 중복 여부 확인하려면? 손으로 못 함. 어느 사이트에서 몇 번째 이미지를 몇 MB를 확장자 뭐였고... 이런 정보들이 될 것.\n",
    "- 크롤링 시 이걸 가지고 페이지 만들 것. 숙제로 낼 것. 하다가 안 되면."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SQL은 내일 진행. 제일 중요. join을 하기 위한 언어 = SQL. 보통 공통적이긴 한데 DBMS마다 지원하는 편차 존재. 문법도 조금 다른 경우 있음. 데이터 타입, 부르는 형태 등 달라서."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
